{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"9.ipynb","provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install pytorch-model-summary"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:13:55.796321Z","iopub.execute_input":"2022-06-07T05:13:55.797281Z","iopub.status.idle":"2022-06-07T05:14:06.828938Z","shell.execute_reply.started":"2022-06-07T05:13:55.79716Z","shell.execute_reply":"2022-06-07T05:14:06.828093Z"},"trusted":true,"id":"Ssoz4Yj91b-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from tqdm import tqdm\n","from pytorch_model_summary import summary\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import FastICA\n","import cv2\n","import warnings\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset,DataLoader\n","from torchvision import transforms\n","from torchvision import models\n","import random\n","from sklearn.utils import shuffle\n","import numpy as np\n","warnings.simplefilter(\"ignore\")"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-07T05:14:06.830796Z","iopub.execute_input":"2022-06-07T05:14:06.831086Z","iopub.status.idle":"2022-06-07T05:14:09.931815Z","shell.execute_reply.started":"2022-06-07T05:14:06.831053Z","shell.execute_reply":"2022-06-07T05:14:09.93072Z"},"trusted":true,"id":"-mtBUmQ71b-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cb3gDtYy0dT","executionInfo":{"status":"ok","timestamp":1657531775115,"user_tz":-330,"elapsed":7371,"user":{"displayName":"Yash Sharma","userId":"04578811976861982751"}},"outputId":"d66d0836-4082-43dd-a5d9-6ed1b70cf0f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting opendatasets\n","  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.64.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2022.6.15)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (6.1.2)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n","Installing collected packages: opendatasets\n","Successfully installed opendatasets-0.1.22\n"]}],"source":["!pip install opendatasets --upgrade"]},{"cell_type":"code","source":["import opendatasets as od"],"metadata":{"id":"QA542li1y2Hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_url = 'https://www.kaggle.com/datasets/vbookshelf/v2-plant-seedlings-dataset'\n","od.download(dataset_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_tTxUY1y_hb","executionInfo":{"status":"ok","timestamp":1657531855992,"user_tz":-330,"elapsed":65067,"user":{"displayName":"Yash Sharma","userId":"04578811976861982751"}},"outputId":"56cd9220-1e1e-4502-b607-d2e89e9da413"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading v2-plant-seedlings-dataset.zip to ./v2-plant-seedlings-dataset\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3.19G/3.19G [00:26<00:00, 131MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["def dataset_to_class_wise_csv(datafolder_path = \"/content/v2-plant-seedlings-dataset\"):\n","    list_of_dir = os.listdir(datafolder_path)\n","    list_of_dir.remove(\"nonsegmentedv2\")\n","    dataset = list()\n","    for index,class_name in enumerate(list_of_dir):\n","        dir_name = os.path.join(datafolder_path , class_name)\n","        list_of_images = os.listdir(dir_name)\n","        for image_name in tqdm(list_of_images):\n","            image_path = os.path.join(dir_name,image_name)\n","            try:\n","                img = cv2.imread(image_path)\n","                img = cv2.resize(img, (256, 256))\n","                red , green , blue = cv2.split(img)\n","                pca = PCA(10)\n","                red_transformed = pca.fit_transform(red)\n","                ica = FastICA(n_components = 5,max_iter = 100)\n","                red_transformed = ica.fit_transform(red)\n","                dataset.append(dict(class_name = class_name,class_id = index,image_path = image_path))\n","            except:\n","                print(\"Currupt Image.\")\n","    dataset = pd.DataFrame(dataset , columns = [\"class_name\",\"class_id\",\"image_path\"])\n","    return dataset\n","dataset = dataset_to_class_wise_csv()\n"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:35:41.989443Z","iopub.execute_input":"2022-06-07T03:35:41.989663Z","iopub.status.idle":"2022-06-07T03:43:20.011705Z","shell.execute_reply.started":"2022-06-07T03:35:41.989638Z","shell.execute_reply":"2022-06-07T03:43:20.010814Z"},"trusted":true,"id":"yp-w6e7H1b-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train val and test split in 70% , 20% , 10%\n","x_train,x_test,y_train,y_test = train_test_split(dataset[\"image_path\"],dataset[\"class_id\"],test_size = 0.1 , stratify = dataset[\"class_id\"])\n","x_train,x_val , y_train,y_val = train_test_split(x_train,y_train,test_size = 0.2 , stratify = y_train)"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:43:22.483001Z","iopub.execute_input":"2022-06-07T03:43:22.483526Z","iopub.status.idle":"2022-06-07T03:43:22.5021Z","shell.execute_reply.started":"2022-06-07T03:43:22.483488Z","shell.execute_reply":"2022-06-07T03:43:22.501458Z"},"trusted":true,"id":"nUjAOxV61b-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def PCA_image_feature_extract(data):\n","    image_features = list()\n","    for image in tqdm(data):\n","        img = cv2.imread(image)\n","        img = cv2.resize(img, (200, 200))\n","        blue , green , red = cv2.split(img)\n","        pca = PCA(5)\n","        red_transformed = pca.fit_transform(red)\n","        green_transformed = pca.fit_transform(green)\n","        blue_transformed = pca.fit_transform(blue)\n","        img_compressed = random.choice([red_transformed,green_transformed,blue_transformed])\n","        img_compressed = img_compressed.flatten()\n","        image_features.append(img_compressed)\n","    image_features = np.stack(image_features , axis = 0)\n","    return image_features\n","def ICA_image_feature_extract(data):\n","    image_features = list()\n","    for image in tqdm(data):\n","        img = cv2.imread(image)\n","        img = cv2.resize(img, (200, 200))\n","        blue , green , red = cv2.split(img)\n","        ica = FastICA(n_components = 5,max_iter = 100)\n","        red_transformed = ica.fit_transform(red)\n","        green_transformed = ica.fit_transform(green)\n","        blue_transformed = ica.fit_transform(blue)\n","        img_compressed = random.choice([red_transformed,green_transformed,blue_transformed])\n","        img_compressed = img_compressed.flatten()\n","        image_features.append(img_compressed)\n","    image_features = np.stack(image_features , axis = 0)\n","    return image_features\n","pca_train_features,pca_test_features,pca_val_features = PCA_image_feature_extract(x_train),PCA_image_feature_extract(x_test),PCA_image_feature_extract(x_val)\n","ica_train_features,ica_test_features,ica_val_features = ICA_image_feature_extract(x_train),ICA_image_feature_extract(x_test),ICA_image_feature_extract(x_val)"],"metadata":{"trusted":true,"id":"erAy8zkY1b-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","with open(\"../input/pca-features/pca_features.pkl\",\"rb\") as file_obj:\n","    pca_train_features,pca_test_features,pca_val_features = pickle.load(file_obj)\n","    file_obj.close()\n","with open(\"../input/ica-features/ica_features.pkl\",\"rb\") as file_obj:\n","    ica_train_features,ica_test_features,ica_val_features = pickle.load(file_obj)\n","    file_obj.close()"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:43:26.664893Z","iopub.execute_input":"2022-06-07T03:43:26.665646Z","iopub.status.idle":"2022-06-07T03:43:28.093288Z","shell.execute_reply.started":"2022-06-07T03:43:26.665604Z","shell.execute_reply":"2022-06-07T03:43:28.092443Z"},"trusted":true,"id":"g7yLlPhC1b-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self,x,y,transforms,pca_features,ica_features):\n","        super(CustomDataset,self).__init__()\n","        self.x = list(x)\n","        self.y = list(y)\n","        self.pca_features = pca_features\n","        self.ica_features = ica_features\n","        self.transforms = transforms\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self,index):\n","        image = Image.open(self.x[index]).convert(\"RGB\")\n","        if self.transforms != None:\n","            image = self.transforms(image)\n","        return image,torch.tensor(self.y[index],dtype = torch.long),torch.tensor(self.pca_features[index],dtype = torch.float32),torch.tensor(self.ica_features[index],dtype = torch.float32)\n","def get_dataloader(x_train,y_train,x_test,y_test,x_val,y_val,\n","                   pca_train_features,pca_test_features,pca_val_features,\n","                   ica_train_features,ica_test_features,ica_val_features,batch_size):\n","    transform = transforms.Compose([transforms.ToTensor(),\n","                                   transforms.Resize((256,256)),\n","                                  transforms.Normalize(mean = (0.485, 0.456, 0.406),std = (0.229, 0.224, 0.225))\n","                                   ])\n","    train_dataset = CustomDataset(x_train,y_train,transform,pca_train_features,ica_train_features)\n","    val_dataset = CustomDataset(x_val,y_val,transform,pca_val_features,ica_val_features)\n","    test_dataset = CustomDataset(x_test , y_test,transform,pca_test_features,ica_test_features)\n","    train_dataloader = DataLoader(dataset = train_dataset , batch_size = batch_size , shuffle = False)\n","    test_dataloader = DataLoader(dataset = test_dataset , batch_size = batch_size , shuffle = False)\n","    val_dataloader = DataLoader(dataset = val_dataset , batch_size = batch_size , shuffle = False)\n","    return train_dataloader , test_dataloader , val_dataloader\n","batch_size = 48\n","train_dataloader , test_dataloader , val_dataloader = get_dataloader(x_train,y_train,x_test,y_test,x_val ,y_val,\n","                                                                     pca_train_features,pca_test_features,pca_val_features,\n","                                                                     ica_train_features,ica_test_features,ica_val_features,batch_size)"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:43:31.008588Z","iopub.execute_input":"2022-06-07T03:43:31.008995Z","iopub.status.idle":"2022-06-07T03:43:31.025678Z","shell.execute_reply.started":"2022-06-07T03:43:31.00896Z","shell.execute_reply":"2022-06-07T03:43:31.024988Z"},"trusted":true,"id":"Yro_trME1b-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model1(nn.Module):\n","    def __init__(self,num_classes):\n","        super(Model1 , self).__init__()\n","        self.num_classes = num_classes\n","        self.resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n","        self.classifier = nn.Sequential(nn.Linear(in_features = 3000,out_features = 256),\n","                                       nn.ReLU(),\n","                                       nn.BatchNorm1d(256),\n","                                       nn.Dropout(p = 0.5),\n","                                       nn.Linear(in_features = 256 , out_features = self.num_classes),\n","                                       nn.Softmax())\n","    def forward(self,x,ml_features1,ml_features2):\n","        resnet_output = self.resnet_model(x)\n","        concatenated_output = torch.cat((resnet_output,ml_features1,ml_features2),axis = 1) # (batch_size , 3000)\n","        final_output = self.classifier(concatenated_output)\n","        return final_output\n","class Model2(nn.Module):\n","    def __init__(self,num_classes):\n","        super(Model2 , self).__init__()\n","        self.num_classes = num_classes\n","        self.googlenet_model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)\n","        self.classifier = nn.Sequential(nn.Linear(in_features = 3000,out_features = 256),\n","                                       nn.ReLU(),\n","                                       nn.BatchNorm1d(256),\n","                                       nn.Dropout(p = 0.5),\n","                                       nn.Linear(in_features = 256 , out_features = self.num_classes),\n","                                       nn.Softmax())\n","    def forward(self,x,ml_features1,ml_features2):\n","        google_net_output = self.googlenet_model(x)\n","        concatenated_output = torch.cat((google_net_output,ml_features1,ml_features2),axis = 1) # (batch_size , 3000)\n","        final_output = self.classifier(concatenated_output)\n","        return final_output\n","class Model3(nn.Module):\n","    def __init__(self,num_classes):\n","        super(Model3 , self).__init__()\n","        self.num_classes = num_classes\n","        self.inception_model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n","        self.inception_model = nn.Sequential(*(list(self.inception_model.children())[:-1]))\n","        self.inception_last_layer = nn.Linear(in_features = 2048 , out_features = 1000)\n","        self.classifier = nn.Sequential(nn.Linear(in_features = 3000,out_features = 512),\n","                                       nn.ReLU(),\n","                                       nn.BatchNorm1d(512),\n","                                       nn.Dropout(p = 0.5),\n","                                       nn.Linear(in_features = 512 , out_features = self.num_classes),\n","                                       nn.Softmax())\n","    def forward(self,x,ml_features1,ml_features2):\n","        inception_output = self.inception_model[0](x)\n","        for layer_no in range(1,20):\n","            if layer_no != 15:\n","                inception_output = self.inception_model[layer_no](inception_output)\n","        inception_output = inception_output.view(inception_output.size(0),-1) #(batch_size , 2048)\n","        inception_output = self.inception_last_layer(inception_output)\n","        concatenated_output = torch.cat((inception_output,ml_features1,ml_features2),axis = 1) # (batch_size , 3000)\n","        final_output = self.classifier(concatenated_output)\n","        return final_output\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:14:09.93333Z","iopub.execute_input":"2022-06-07T05:14:09.933629Z","iopub.status.idle":"2022-06-07T05:14:09.954792Z","shell.execute_reply.started":"2022-06-07T05:14:09.933597Z","shell.execute_reply":"2022-06-07T05:14:09.953804Z"},"trusted":true,"id":"wqAHIfUs1b-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(summary(Model1(num_classes=12), torch.zeros((1, 3, 256, 256)), torch.zeros((1,1000)),torch.zeros((1,1000)) ,show_input=True))\n","print(summary(Model2(num_classes=12), torch.zeros((1, 3, 256, 256)), torch.zeros((1,1000)),torch.zeros((1,1000)) ,show_input=True))\n","print(summary(Model3(num_classes=12), torch.zeros((1, 3, 256, 256)), torch.zeros((1,1000)),torch.zeros((1,1000)) ,show_input=True))"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:14:09.956893Z","iopub.execute_input":"2022-06-07T05:14:09.957125Z","iopub.status.idle":"2022-06-07T05:14:36.901145Z","shell.execute_reply.started":"2022-06-07T05:14:09.957098Z","shell.execute_reply":"2022-06-07T05:14:36.90005Z"},"trusted":true,"id":"F_ZT6tik1b-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Train_DL_Model(object):\n","    def __init__(self,train_dataloader,test_dataloader,validation_dataloader,\n","                 model,n_epochs,n_classes,lr = 0.00001):\n","        self.train_dataloader = train_dataloader\n","        self.test_dataloader = test_dataloader\n","        self.validation_dataloader = validation_dataloader\n","        self.n_classes = n_classes\n","        self.n_epochs = n_epochs\n","        self.lr = lr\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model = model(self.n_classes).to(self.device)\n","        self.optimizer = torch.optim.Adam(self.model.parameters() , lr = self.lr)\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.train_prediction = []\n","        self.validation_prediction = []\n","        self.test_prediction = []\n","        self.train_original = []\n","        self.test_original = []\n","        self.val_original = []\n","    def train_model(self):\n","        print(\"Training Start...\")\n","        num_batch = len(self.train_dataloader)\n","        for epoch in range(self.n_epochs):\n","            print(\"| ********* Epoch : {} ********** |\".format(epoch+1))\n","            for index,(image,label,pca_features,ica_features) in enumerate(self.train_dataloader):\n","                image = image.to(self.device)\n","                label = label.to(self.device)\n","                ica_features = ica_features.to(self.device)\n","                pca_features = pca_features.to(self.device)\n","                self.optimizer.zero_grad()\n","                output = self.model(image,ica_features,pca_features)\n","                loss = self.criterion(output , label)\n","                loss.backward()\n","                self.optimizer.step()\n","                if (index) % 10 == 0:\n","                    print(\"| Batch : {0}/{1} |\".format(index, num_batch))\n","        print(\"Training End .....\")\n","    def get_test_validation_prediction(self,dataloader):\n","        num_batch = len(dataloader)\n","        with torch.no_grad():\n","            total_prediction = []\n","            total_original = []\n","            probability= []\n","            for index,(image,label,pca_features,ica_features) in enumerate(dataloader):\n","                image = image.to(self.device)\n","                ica_features = ica_features.to(self.device)\n","                pca_features = pca_features.to(self.device)\n","                output = self.model(image,ica_features,pca_features)\n","                prediction = torch.argmax(output , axis = 1).cpu().tolist()\n","                total_original.extend(label.tolist())\n","                total_prediction.extend(prediction)\n","                probability.extend(output.cpu().tolist())\n","                if (index) % 10 == 0:\n","                    print(\"| Batch : {0}/{1} |\".format(index, num_batch))\n","        return total_prediction,total_original,probability\n","    def test_validation(self):\n","        print(\"Training Prediction Start \")\n","        pred , original,train_probability = self.get_test_validation_prediction(self.train_dataloader)\n","        self.train_prediction.extend(pred)\n","        self.train_original.extend(original)\n","        print(\"Training Prediction End \")\n","        print(\"Validation Prediction Start \")\n","        pred , original, val_probability= self.get_test_validation_prediction(self.validation_dataloader)\n","        self.validation_prediction.extend(pred)\n","        self.val_original.extend(original)\n","        print(\"Validation Prediction End \")\n","        print(\"Testing Prediction Start \")\n","        pred , original ,test_probability= self.get_test_validation_prediction(self.test_dataloader)\n","        self.test_prediction.extend(pred)\n","        self.test_original.extend(original)\n","        print(\"Validation Prediction End \")\n","        return (self.train_prediction,self.validation_prediction , self.test_prediction),(self.train_original,self.val_original,self.test_original),(train_probability,val_probability,test_probability)\n"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:43:36.971065Z","iopub.execute_input":"2022-06-07T03:43:36.971699Z","iopub.status.idle":"2022-06-07T03:43:36.990833Z","shell.execute_reply.started":"2022-06-07T03:43:36.971653Z","shell.execute_reply":"2022-06-07T03:43:36.990174Z"},"trusted":true,"id":"He9m5xS-1b-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score , f1_score,recall_score,precision_score,classification_report\n","import pickle\n","def calulate_result(original_data,DL):\n","    accuracy = dict(train = accuracy_score(original_data[0],DL[0])*100,\n","                   val = accuracy_score(original_data[1],DL[1])*100,\n","                   test = accuracy_score(original_data[2],DL[2])*100)\n","    print(\"accuracy\", accuracy)\n","    recall = dict(train = recall_score(original_data[0],DL[0],average = \"weighted\")*100,\n","                   val = recall_score(original_data[1],DL[1],average = \"weighted\")*100,\n","                   test = recall_score(original_data[2],DL[2],average = \"weighted\")*100)\n","    print(\"recall\", recall)\n","    precision = dict(train = precision_score(original_data[0],DL[0],average = \"weighted\")*100,\n","                   val = precision_score(original_data[1],DL[1],average = \"weighted\")*100,\n","                   test = precision_score(original_data[2],DL[2],average = \"weighted\")*100)\n","    print(\"precision\", precision)\n","    f1 = dict(train = f1_score(original_data[0],DL[0],average = \"weighted\")*100,\n","                   val = f1_score(original_data[1],DL[1],average = \"weighted\")*100,\n","                   test = f1_score(original_data[2],DL[2],average = \"weighted\")*100)\n","    print(\"f1\", f1)\n","    report = dict(train = classification_report(original_data[0],DL[0],output_dict = True),\n","                   val = classification_report(original_data[1],DL[1],output_dict = True),\n","                   test = classification_report(original_data[2],DL[2],output_dict = True))"],"metadata":{"id":"Kb9UhA2RmFSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","from sklearn.metrics import precision_recall_curve, roc_curve\n","def plot_curve(original_data, DL, title):\n","  y_7 = pd.Series(original_data)\n","  new_labels = {7:1, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_7 = y_7.map(new_labels)\n","  y_7 = y_7.to_numpy()\n","\n","  y_5 = pd.Series(original_data)\n","  new_labels = {7:0, 5:1, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_5 = y_5.map(new_labels)\n","  y_5 = y_5.to_numpy()\n","\n","  y_8 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:1, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_8 = y_8.map(new_labels)\n","  y_8 = y_8.to_numpy()\n","\n","  y_6 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:1, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_6 = y_6.map(new_labels)\n","  y_6 = y_6.to_numpy()\n","\n","  y_11 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:1, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_11 = y_11.map(new_labels)\n","  y_11 = y_11.to_numpy()\n","\n","  y_1 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:1, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_1 = y_1.map(new_labels)\n","  y_1 = y_1.to_numpy()\n","\n","  y_4 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:1, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_4 = y_4.map(new_labels)\n","  y_4 = y_4.to_numpy()\n","\n","  y_9 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:1, 2:0, 10:0, 3:0, 0:0}\n","  y_9 = y_9.map(new_labels)\n","  y_9 = y_9.to_numpy()\n","\n","  y_2 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:1, 10:0, 3:0, 0:0}\n","  y_2 = y_2.map(new_labels)\n","  y_2 = y_2.to_numpy()\n","\n","  y_10 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:1, 3:0, 0:0}\n","  y_10 = y_10.map(new_labels)\n","  y_10 = y_10.to_numpy()\n","\n","  y_3 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:1, 0:0}\n","  y_3 = y_3.map(new_labels)\n","  y_3 = y_3.to_numpy() \n","\n","  y_0 = pd.Series(original_data)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:1}\n","  y_0 = y_0.map(new_labels)\n","  y_0 = y_0.to_numpy()\n","\n","  y_7_pred = pd.Series(DL)\n","  new_labels = {7:1, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_7_pred = y_7_pred.map(new_labels)\n","  y_7_pred = y_7_pred.to_numpy()\n","\n","  y_5_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:1, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_5_pred = y_5_pred.map(new_labels)\n","  y_5_pred = y_5_pred.to_numpy()\n","\n","  y_8_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:1, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_8_pred = y_8_pred.map(new_labels)\n","  y_8_pred = y_8_pred.to_numpy()\n","\n","  y_6_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:1, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_6_pred = y_6_pred.map(new_labels)\n","  y_6_pred = y_6_pred.to_numpy()\n","\n","  y_11_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:1, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_11_pred = y_11_pred.map(new_labels)\n","  y_11_pred = y_11_pred.to_numpy()\n","\n","  y_1_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:1, 4:0, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_1_pred = y_1_pred.map(new_labels)\n","  y_1_pred = y_1_pred.to_numpy()\n","\n","  y_4_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:1, 9:0, 2:0, 10:0, 3:0, 0:0}\n","  y_4_pred = y_4_pred.map(new_labels)\n","  y_4_pred = y_4_pred.to_numpy()\n","\n","  y_9_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:1, 2:0, 10:0, 3:0, 0:0}\n","  y_9_pred = y_9_pred.map(new_labels)\n","  y_9_pred = y_9_pred.to_numpy()\n","\n","  y_2_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:1, 10:0, 3:0, 0:0}\n","  y_2_pred = y_2_pred.map(new_labels)\n","  y_2_pred = y_2_pred.to_numpy()\n","\n","  y_10_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:1, 3:0, 0:0}\n","  y_10_pred = y_10_pred.map(new_labels)\n","  y_10_pred = y_10_pred.to_numpy()\n","\n","  y_3_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:1, 0:0}\n","  y_3_pred = y_3_pred.map(new_labels)\n","  y_3_pred = y_3_pred.to_numpy() \n","\n","  y_0_pred = pd.Series(DL)\n","  new_labels = {7:0, 5:0, 8:0, 6:0, 11:0, 1:0, 4:0, 9:0, 2:0, 10:0, 3:0, 0:1}\n","  y_0_pred = y_0_pred.map(new_labels)\n","  y_0_pred = y_0_pred.to_numpy()\n","\n","  precision_c0, recall_c0, _ = precision_recall_curve(y_0, y_0_pred)\n","  precision_c1, recall_c1, _ = precision_recall_curve(y_1, y_1_pred)\n","  precision_c2, recall_c2, _ = precision_recall_curve(y_2, y_2_pred)\n","  precision_c3, recall_c3, _ = precision_recall_curve(y_3, y_3_pred)\n","  precision_c4, recall_c4, _ = precision_recall_curve(y_4, y_4_pred)\n","  precision_c5, recall_c5, _ = precision_recall_curve(y_5, y_5_pred)\n","  precision_c6, recall_c6, _ = precision_recall_curve(y_6, y_6_pred)\n","  precision_c7, recall_c7, _ = precision_recall_curve(y_7, y_7_pred)\n","  precision_c8, recall_c8, _ = precision_recall_curve(y_8, y_8_pred)\n","  precision_c9, recall_c9, _ = precision_recall_curve(y_9, y_9_pred)\n","  precision_c10, recall_c10, _ = precision_recall_curve(y_10, y_10_pred)\n","  precision_c11, recall_c11, _ = precision_recall_curve(y_11, y_11_pred)\n","  \n","  fpr0, tpr0, _ = roc_curve(y_0, y_0_pred)\n","  fpr1, tpr1, _ = roc_curve(y_1, y_1_pred)\n","  fpr2, tpr2, _ = roc_curve(y_2, y_2_pred)\n","  fpr3, tpr3, _ = roc_curve(y_3, y_3_pred)\n","  fpr4, tpr4, _ = roc_curve(y_4, y_4_pred)\n","  fpr5, tpr5, _ = roc_curve(y_5, y_5_pred)\n","  fpr6, tpr6, _ = roc_curve(y_6, y_6_pred)\n","  fpr7, tpr7, _ = roc_curve(y_7, y_7_pred)\n","  fpr8, tpr8, _ = roc_curve(y_8, y_8_pred)\n","  fpr9, tpr9, _ = roc_curve(y_9, y_9_pred)\n","  fpr10, tpr10, _ = roc_curve(y_10, y_10_pred)\n","  fpr11, tpr11, _ = roc_curve(y_11, y_11_pred)\n","  \n","  #plot precision-recall curve\n","  plt.plot(precision_c0, recall_c0)\n","  plt.plot(precision_c1, recall_c1)\n","  plt.plot(precision_c2, recall_c2)\n","  plt.plot(precision_c3, recall_c3)\n","  plt.plot(precision_c4, recall_c4)\n","  plt.plot(precision_c5, recall_c5)\n","  plt.plot(precision_c6, recall_c6)\n","  plt.plot(precision_c7, recall_c7)\n","  plt.plot(precision_c8, recall_c8)\n","  plt.plot(precision_c9, recall_c9)\n","  plt.plot(precision_c10, recall_c10)\n","  plt.plot(precision_c11, recall_c11)\n","  plt.legend([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"], loc =\"lower right\")\n","  plt.title(title)\n","  plt.plot([0, 1], [0, 1],'r--')\n","  plt.xlim([0, 1])\n","  plt.ylim([0, 1])\n","  plt.xlabel('Precision')\n","  plt.ylabel('Recall')\n","  plt.show()\n","  plt.savefig(\"PR-AUC-merged-BiGRUx3.png\", dpi = 200)\n","\n","  # Plot ROC AUC\n","  plt.plot(fpr0, tpr0)\n","  plt.plot(fpr1, tpr1)\n","  plt.plot(fpr2, tpr2)\n","  plt.plot(fpr3, tpr3)\n","  plt.plot(fpr4, tpr4)\n","  plt.plot(fpr5, tpr5)\n","  plt.plot(fpr6, tpr6)\n","  plt.plot(fpr7, tpr7)\n","  plt.plot(fpr8, tpr8)\n","  plt.plot(fpr9, tpr9)\n","  plt.plot(fpr10, tpr10)\n","  plt.plot(fpr11, tpr11)\n","  plt.legend([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"], loc =\"lower right\")\n","  plt.title(title)\n","  plt.plot([0, 1], [0, 1],'r--')\n","  plt.xlim([0, 1])\n","  plt.ylim([0, 1])\n","  plt.ylabel('True Positive Rate')\n","  plt.xlabel('False Positive Rate')\n","  plt.show()\n","  plt.savefig(\"ROC-AUC-merged-BiGRUx3.png\", dpi = 200)"],"metadata":{"id":"xgRWttjm0u4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train Model 1\n","n_epochs = 1\n","model1 = Train_DL_Model(train_dataloader,test_dataloader,val_dataloader,Model1,n_epochs,12)\n","model1.train_model()\n","model1_pred,model1_original,pred_probability1 = model1.test_validation()"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:43:47.129484Z","iopub.execute_input":"2022-06-07T03:43:47.129739Z","iopub.status.idle":"2022-06-07T03:45:58.877459Z","shell.execute_reply.started":"2022-06-07T03:43:47.129711Z","shell.execute_reply":"2022-06-07T03:45:58.876644Z"},"trusted":true,"id":"G5vDxiB91b-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calulate_result(model1_original,model1_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yfWT2EhCmIuF","executionInfo":{"status":"ok","timestamp":1657533209576,"user_tz":-330,"elapsed":17,"user":{"displayName":"Yash Sharma","userId":"04578811976861982751"}},"outputId":"ede36167-b83b-4f3a-db37-332ae5105c8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy {'train': 9.979939819458375, 'val': 8.826479438314944, 'test': 11.191335740072201}\n","recall {'train': 9.979939819458375, 'val': 8.826479438314944, 'test': 11.191335740072201}\n","precision {'train': 10.913101434169551, 'val': 10.031375420901007, 'test': 11.92452856381401}\n","f1 {'train': 10.244326663489261, 'val': 9.15710242160189, 'test': 11.28641164517959}\n"]}]},{"cell_type":"code","source":["plot_curve(model1_original[0], model1_pred[0], 'Train')\n","plot_curve(model1_original[1], model1_pred[1], 'Valiadation')\n","plot_curve(model1_original[2], model1_pred[2], 'Test')"],"metadata":{"id":"_MBDKe03034b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train Model 2\n","n_epochs = 1\n","model2 = Train_DL_Model(train_dataloader,test_dataloader,val_dataloader,Model2,n_epochs,12)\n","model2.train_model()\n","model2_pred,model2_original,pred_probability2 = model2.test_validation()"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:45:58.879092Z","iopub.execute_input":"2022-06-07T03:45:58.879485Z","iopub.status.idle":"2022-06-07T03:48:11.609674Z","shell.execute_reply.started":"2022-06-07T03:45:58.879442Z","shell.execute_reply":"2022-06-07T03:48:11.608958Z"},"trusted":true,"id":"do2HqbQN1b-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calulate_result(model2_original,model2_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2Ldq9afmLMb","executionInfo":{"status":"ok","timestamp":1657533337141,"user_tz":-330,"elapsed":21,"user":{"displayName":"Yash Sharma","userId":"04578811976861982751"}},"outputId":"c5092f21-ad16-4c95-c393-9d40e644c1f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy {'train': 9.804413239719159, 'val': 7.823470411233702, 'test': 8.303249097472925}\n","recall {'train': 9.804413239719159, 'val': 7.823470411233702, 'test': 8.303249097472925}\n","precision {'train': 10.324424651992063, 'val': 8.15242953492156, 'test': 8.910496540495181}\n","f1 {'train': 9.697134047202235, 'val': 7.7674269965346046, 'test': 8.298553614452116}\n"]}]},{"cell_type":"code","source":["plot_curve(model2_original[0], model2_pred[0], 'Train')\n","plot_curve(model2_original[1], model2_pred[1], 'Valiadation')\n","plot_curve(model2_original[2], model2_pred[2], 'Test')"],"metadata":{"id":"4H8XPotr0422"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train Model 3\n","n_epochs = 1\n","model3 = Train_DL_Model(train_dataloader,test_dataloader,val_dataloader,Model3,n_epochs,12)\n","model3.train_model()\n","model3_pred,model3_original,pred_probability3 = model3.test_validation()"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:48:11.611024Z","iopub.execute_input":"2022-06-07T03:48:11.611442Z","iopub.status.idle":"2022-06-07T03:50:43.885824Z","shell.execute_reply.started":"2022-06-07T03:48:11.611405Z","shell.execute_reply":"2022-06-07T03:50:43.885104Z"},"trusted":true,"id":"sNsbHuYX1b-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calulate_result(model3_original,model3_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3VVOb3qmNDL","executionInfo":{"status":"ok","timestamp":1657533494437,"user_tz":-330,"elapsed":13,"user":{"displayName":"Yash Sharma","userId":"04578811976861982751"}},"outputId":"3cfd0158-9cc8-4e29-cad9-494746091c5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy {'train': 9.804413239719159, 'val': 9.42828485456369, 'test': 9.025270758122744}\n","recall {'train': 9.804413239719159, 'val': 9.42828485456369, 'test': 9.025270758122744}\n","precision {'train': 10.91769421415426, 'val': 10.986131477757672, 'test': 10.28557426765606}\n","f1 {'train': 10.084656929224273, 'val': 9.840005177516067, 'test': 9.27162931057184}\n"]}]},{"cell_type":"code","source":["plot_curve(model3_original[0], model3_pred[0], 'Train')\n","plot_curve(model3_original[1], model3_pred[1], 'Valiadation')\n","plot_curve(model3_original[2], model3_pred[2], 'Test')"],"metadata":{"id":"i4CPsbdJ05zJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","train_prob = []\n","for model1,model2,model3,p1,p2,p3 in zip(model1_pred[0],model2_pred[0],model3_pred[0],pred_probability1[0],pred_probability2[0],pred_probability3[0]):\n","    counter = Counter([model1,model2,model3])\n","    counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)[0][0]\n","    if counter == model1:\n","        train_prob.append(p1)\n","    elif counter == model2:\n","        train_prob.append(p2)\n","    elif counter == model3:\n","        train_prob.append(p3)\n","# validation prediction\n","val_prob = []\n","for model1,model2,model3,p1,p2,p3 in zip(model1_pred[1],model2_pred[1],model3_pred[1],pred_probability1[1],pred_probability2[1],pred_probability3[1]):\n","    counter = Counter([model1,model2,model3])\n","    counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)[0][0]\n","    if counter == model1:\n","        val_prob.append(p1)\n","    elif counter == model2:\n","        val_prob.append(p2)\n","    elif counter == model3:\n","        val_prob.append(p3)\n","# test prediction\n","test_prob = []\n","for model1,model2,model3,p1,p2,p3 in zip(model1_pred[2],model2_pred[2],model3_pred[2],pred_probability1[2],pred_probability2[2],pred_probability3[2]):\n","    counter = Counter([model1,model2,model3])\n","    counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)[0][0]\n","    if counter == model1:\n","        test_prob.append(p1)\n","    elif counter == model2:\n","        test_prob.append(p2)\n","    elif counter == model3:\n","        test_prob.append(p3)"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:52:56.318134Z","iopub.execute_input":"2022-06-07T03:52:56.318393Z","iopub.status.idle":"2022-06-07T03:52:56.367826Z","shell.execute_reply.started":"2022-06-07T03:52:56.318363Z","shell.execute_reply":"2022-06-07T03:52:56.36719Z"},"trusted":true,"id":"6xlfMNIu1b-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","from sklearn.metrics import accuracy_score , f1_score,recall_score,precision_score,classification_report\n","import pickle\n","def calulate_result(original_data,Model1,Model2,Model3,pred_probability):\n","    # train_prediction\n","    train_prediction = []\n","    for model1,model2,model3 in zip(Model1[0],Model2[0],Model3[0]):\n","        counter = Counter([model1,model2,model3])\n","        counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)[0][0]\n","        train_prediction.append(counter)\n","    # validation prediction\n","    val_prediction = []\n","    for model1,model2,model3 in zip(Model1[1],Model2[1],Model3[1]):\n","        counter = Counter([model1,model2,model3])\n","        counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)[0][0]\n","        val_prediction.append(counter)\n","    # test prediction\n","    test_prediction = []\n","    for model1,model2,model3 in zip(Model1[2],Model2[2],Model3[2]):\n","        counter = Counter([model1,model2,model3])\n","        counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)[0][0]\n","        test_prediction.append(counter)\n","    accuracy = dict(train = accuracy_score(original_data[0],train_prediction)*100,\n","                   val = accuracy_score(original_data[1],val_prediction)*100,\n","                   test = accuracy_score(original_data[2],test_prediction)*100)\n","    print(\"accuracy\", accuracy)\n","    recall = dict(train = recall_score(original_data[0],train_prediction,average = \"weighted\")*100,\n","                   val = recall_score(original_data[1],val_prediction,average = \"weighted\")*100,\n","                   test = recall_score(original_data[2],test_prediction,average = \"weighted\")*100)\n","    print(\"recall\", recall)\n","    precision = dict(train = precision_score(original_data[0],train_prediction,average = \"weighted\")*100,\n","                   val = precision_score(original_data[1],val_prediction,average = \"weighted\")*100,\n","                   test = precision_score(original_data[2],test_prediction,average = \"weighted\")*100)\n","    print(\"precision\", precision)\n","    f1 = dict(train = f1_score(original_data[0],train_prediction,average = \"weighted\")*100,\n","                   val = f1_score(original_data[1],val_prediction,average = \"weighted\")*100,\n","                   test = f1_score(original_data[2],test_prediction,average = \"weighted\")*100)\n","    print(\"f1\", f1)\n","    report = dict(train = classification_report(original_data[0],train_prediction,output_dict = True),\n","                   val = classification_report(original_data[1],val_prediction,output_dict = True),\n","                   test = classification_report(original_data[2],test_prediction,output_dict = True))\n","    with open(\"Architecure_9_result.pkl\" , \"wb\") as file_obj:\n","        data = dict(accuracy = accuracy,recall = recall,precision = precision,f1 = f1,report = report,name = \"Architecure_9_result\",\n","                   original_data = original_data , model_prediction = [train_prediction,val_prediction,test_prediction],probability = pred_probability)\n","        pickle.dump(data , file_obj)\n","calulate_result(model1_original,model1_pred,model2_pred,model3_pred,(train_prob,val_prob,test_prob))"],"metadata":{"execution":{"iopub.status.busy":"2022-06-07T03:53:13.695673Z","iopub.execute_input":"2022-06-07T03:53:13.698275Z","iopub.status.idle":"2022-06-07T03:53:13.868249Z","shell.execute_reply.started":"2022-06-07T03:53:13.698233Z","shell.execute_reply":"2022-06-07T03:53:13.867509Z"},"trusted":true,"id":"dsvhXjhr1b-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"trusted":true,"id":"GXr23W_r1b-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_probability , val_probability , test_probability =np.array(train_prob),np.array(val_prob),np.array(test_prob)\n","y_train,y_val,y_test = np.array(model3_original[0]),np.array(model3_original[1]),np.array(model3_original[2])"],"metadata":{"trusted":true,"id":"itcF37YW1b-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","from itertools import cycle\n","import matplotlib.pyplot as plt"],"metadata":{"trusted":true,"id":"toIICBcw1b-j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TRAIN ROC"],"metadata":{"id":"hgyCvYT71b-j"}},{"cell_type":"code","source":["fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","lw=2\n","n_classes = 12\n","one_hot = np.zeros((y_train.shape[0],n_classes))\n","for index,label in enumerate(y_train):\n","    one_hot[index][label] = 1\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(one_hot[:, i], train_probability[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","for i in range(n_classes):\n","    plt.plot(fpr[i], tpr[i],  lw=2,\n","             label='ROC curve of class {0} (area = {1:0.2f})'\n","             ''.format(i, roc_auc[i]))\n","plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n","plt.xlim([-0.05, 1.0])\n","plt.ylim([0.0, 1])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic for multi-class data')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"trusted":true,"id":"GRiVlqGj1b-l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# VAL ROC"],"metadata":{"id":"YSzD8coy1b-m"}},{"cell_type":"code","source":["fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","lw=2\n","n_classes = 12\n","one_hot = np.zeros((y_val.shape[0],n_classes))\n","for index,label in enumerate(y_val):\n","    one_hot[index][label] = 1\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(one_hot[:, i], val_probability[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","for i in range(n_classes):\n","    plt.plot(fpr[i], tpr[i],  lw=2,\n","             label='ROC curve of class {0} (area = {1:0.2f})'\n","             ''.format(i, roc_auc[i]))\n","plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n","plt.xlim([-0.05, 1.0])\n","plt.ylim([0.0, 1])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic for multi-class data')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"trusted":true,"id":"YvbpY11C1b-m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TEST ROC"],"metadata":{"id":"-MJRjLgc1b-m"}},{"cell_type":"code","source":["fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","lw=2\n","n_classes = 12\n","one_hot = np.zeros((y_test.shape[0],n_classes))\n","for index,label in enumerate(y_test):\n","    one_hot[index][label] = 1\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(one_hot[:, i], test_probability[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","for i in range(n_classes):\n","    plt.plot(fpr[i], tpr[i],  lw=2,\n","             label='ROC curve of class {0} (area = {1:0.2f})'\n","             ''.format(i, roc_auc[i]))\n","plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n","plt.xlim([-0.05, 1.0])\n","plt.ylim([0.0, 1])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic for multi-class data')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"trusted":true,"id":"6g3hGHz61b-n"},"execution_count":null,"outputs":[]}]}